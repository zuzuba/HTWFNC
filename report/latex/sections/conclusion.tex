\section{Conclusions}
We presented an optimized implementation of 4-bits quantization and 4-bits quantized forward prediction for Neural Networks. Our experimental results show the benefits of performing vectorization and blocking to achieve significant speed-ups over a naive implementation. The key ingredient in our approach lies in the design of the data structure $\emph{uint4x4\_t}.$ In particular it allows us to easily transfer optimization techniques for standard float MMM to QMMM, such as blocking.
Moreover, we showed that, despite the fact that the bulk of the computation happens within the kernel, vectorizing functions that support it can yield a substantial improvement.

%decision to map a tile of $2 \times 2$ floats to a single 16-bit integer (using a struct of 4 members with 4 reserved bits respectively). Through this construction, the algorithms outlined in the previous are accessible in a convenient form that lends itself to simply vectorize the load, quantize, round and saturate functions, while also enabling a simplified blocking scheme for quantized matrix multiplication.