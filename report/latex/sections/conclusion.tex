\section{Conclusions}
The experimental results presented in the previous sections outline
the benefits of performing vectorization and matrix blocking to achieve
a significant speedup over a naive implementation. The key ingredient in
the approach outlined above lies in the decision to map a tile of $2 \times 2$
floats to a single 16-bit integer (using a struct of 4 members with 4 reserved bits
respectively). Through this construction, the algorithms outlined in the previous are accessible
in a convenient form that lends itself to simply vectorize the load, quantize, round and saturate functions,
while also enabling a simplified blocking scheme for quantized matrix multiplication. More importantly,
each of the individual optimizations performed demonstrate the ability to improve over the current optimized
implementations of the General Matrix Multiplication with Low Precision library developed by Google. Although
this library is able to achieve overall better results due to the inclusion of auto-tuning approaches, it is possible
that through inclusion of our approaches, the store, load, quantization and subsequently the auto-tuning approaches could
achieve better overall results. On the other hand, the quantization process only needs to be performed once, and
it is not necessary for quantization to be performed on an embedded device, so the benefit of including the work performed
in this project is of minute importance in a general setting. 
%However, envisioning a scenario where devices
%with reduced memory capacity receive weights to neural networks from an external source (along with precomputed minimum
%and maximum values for each matrix). In such a scenario, the work done in this project would enable each of those devices to perform quantization
%locally and store the compressed weights as 4-bit integers and subsequently perform inference locally. This is particularly useful in a scenario where
%a distributed set of nodes are required to quantize weights to the neural network in different formats, and a central entity provides these weights
%as floating point numbers.
%
%Here you need to briefly summarize what you did and why this is
%important. {\em Do not take the abstract} and put it in the past
%tense. Remember, now the reader has (hopefully) read the paper, so it
%is a very different situation from the abstract. Try to highlight
%important results and say the things you really want to get across
%(e.g., the results show that we are within 2x of the optimal performance ... 
%Even though we only considered the DFT, our optimization
%techniques should be also applicable ....) You can also formulate next
%steps if you want. Be brief.