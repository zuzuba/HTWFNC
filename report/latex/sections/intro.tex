
\section{Introduction}\label{sec:intro}
%In this section we provide an overview of the impact of neural networks (NNs) across multiple scientific disciplines. We argue about the importance of their performance and we discuss previous work that has been done in the field.
%
%\mypar{Motivation}
In recent years we are witnessing an exponential increase in the amount of data available for analysis in almost all scientific disciplines. As a result, there is a high interest in machine learning methods to conduct such analysis. Among these, Neural Networks (NNs) are regarded as one of the most promising techniques. 
They have been successfully applied to a wide range of tasks including medical applications \cite{amato_artificial_2013}, image recognition \cite{krizhevsky_imagenet_2012} and robotics \cite{gu_deep_2016}. 

One of the main drawbacks of NNs is their high number of parameters. As a consequence, NNs require a lot of memory resources for storage and a lot of computational resources for training and forward prediction. While the training phase is usually performed on parallel computing architectures where there is an abundance of both computational and memory resources, the platforms where  trained networks are deployed usually have more limited capabilities (e.g. mobile phones). This problem has steered attention of the research community toward reducing the memory and computation requirements for trained NNs.

A promising research direction is the one of quantized neural networks (QNNs). The central idea to QNNs is to compress the parameters of the network from their float representation to a light-weight one based on quantization bins. The parameter space is divided into a predefined number of bins and each parameter float value is mapped to a bin. The number of bins trades-off the accuracy versus the gain in memory and computation requirements. 
%By ordering the bins, there is a convenient bijective relation between them and the natural numbers. This allows us to represent the approximation of the initial four byte floats with an integer that requires $ceil(\log_2({num~of~bins}))$ bits. This leads to an obvious improvement in memory requirements as long as the number of bins is smaller than $2^{32}$. Furthermore, by fitting more operands in AVX registers and by exploiting spatial locality in caches, quantization results in reduced computational cost for forward prediction.

In this work we present an optimized implementation of a QNN for the forward prediction on the MNIST data set that makes use of fours bits quantization. 

% The first task is to motivate what you do.  You can
%start general and zoom in one the specific problem you consider.  In
%the process you should have explained to the reader: what you are doing,
%why you are doing, why it is important (order is usually reversed).
%
%For example, if my result is the fastest DFT implementation ever, one
%could roughly go as follows. First explain why the DFT is important
%(used everywhere with a few examples) and why performance matters (large datasets,
%realtime). Then explain that fast implementations are very hard and
%expensive to get (memory hierarchy, vector, parallel). 
%
%Now you state what you do in this paper. In our example: 
%presenting a DFT implementation that is
%faster for some sizes than all the other ones.

\mypar{Related work} The research regarding NNs compression focuses mostly on memory requirements. Thus the performance benefits that can be obtained as a by-product of compression are almost  unexplored. Hence   contributions in the literatue are mostly on the algorithmic side rather than on the code optimization one. For example, \cite{gong_compressing_2014} use $k$-means clustering to reduce the size of  a convolutional NN. The work of \cite{chen_compressing_2015} exploits the over-parametrization of NNs by randomly grouping parameters by means of a hashing function. In \cite{denton_exploiting_2014} the authors reduce the size of the convolution filters of a NN using low-ranking approximation methods.	More closely related to our method are the works that explicitly reduce the number of bits used to represent the weights in a NN. Among these \cite{he_effective_2016} propose a high-accuracy four bits quantization scheme for recurrent NNs, a type of NN that is notorious for low prediction performance when quantized. However, their open source implementation is not optimized for performance. In \cite{hubara_binarized_2016} present a high-performance implementation of one bit QNNs optimized for Graphical Processing Units (GPUs) is presented. The authors of \cite{vanhoucke_improving_2011} introduce an implementation of an eight bits QNN for speech recognition optimized for CPUs.

Our contribution consists in an high-performance implementation of four bits QNN optimized for CPUs. While this level of quantization can yield substantial improvements in memory and computation requirements, it presents implementation challenges due to the byte addressability of most computer memories and to the lack of built-in data type for four bits integers.


%\mypar{Related work} Next, you have to give a brief overview of
%related work. For a paper like this, anywhere between 2 and 8
%references. Briefly explain what they do. In the end contrast to what
%you do to make now precisely clear what your contribution is.
